{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea9c92c0",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a62e61",
   "metadata": {},
   "source": [
    "This is the filed of ML that focuses on creating models from a text data sources. That means all articles and availabe words!\n",
    "\n",
    "This is extremaly large field of ML! We are just covering the small portion of it. \n",
    "\n",
    "More information can be found at Wikipedia Article on NLP. NLTK Book (Python library) that is not contained in PySpark. \n",
    "And finally the Foundations of Statistical Natural Language Processing (Manning). \n",
    "\n",
    "Examples of NLP: \n",
    "\n",
    "- Clustering News Articles\n",
    "- Suggesting similair books\n",
    "- Grouping Legal Docs\n",
    "- Analyzing Consumer Feedback\n",
    "- Spam Email Detection \n",
    "\n",
    "Basic process for NLP task concerns: \n",
    "\n",
    "- Compile all documents (Corpus)\n",
    "- Featurize the words to numerics\n",
    "- Compare features of documents\n",
    "\n",
    "We can distingush more complex or complicated ML algorithms like Words to Vector or WordToVec that is analyzing every sentence separetely. \n",
    "We are focusing on classic NLP here.\n",
    "\n",
    "The mechanism behind the NLP is based on TF-IDF method. \n",
    "That stands from Term Frequency - Inverse Document Frequency.\n",
    "\n",
    "Basicly it is creating \"Bag of Words\". Meaning it creates a word count vector array from the number of words showing in the particular doc. Again a bag of counted words, the most simple :) \n",
    "\n",
    "We can improve on Bag of Words by adjusting word counts based on their frequency in corpus(the group of all thedocuments). \n",
    "\n",
    "We can however improve on the TF-IDF method a lot!\n",
    "\n",
    "We end up doing is taking: \n",
    "\n",
    "- Term Frequency - Importance of the termin within the documnent. \n",
    " - TF(x,y) = Number of occurences of term x in document y\n",
    " \n",
    "- Inverse Document Frequency -\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79756887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/ubuntu/Spark/spark-3.3.0-bin-hadoop3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fe8bdc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/16 11:14:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/09/16 11:14:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('NLP').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41b656c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
