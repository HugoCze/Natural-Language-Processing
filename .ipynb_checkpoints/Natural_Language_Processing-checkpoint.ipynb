{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea9c92c0",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a62e61",
   "metadata": {},
   "source": [
    "This is the filed of ML that focuses on creating models from a text data sources. That means all articles and availabe words!\n",
    "\n",
    "This is extremaly large field of ML! We are just covering the small portion of it. \n",
    "\n",
    "More information can be found at Wikipedia Article on NLP. NLTK Book (Python library) that is not contained in PySpark. \n",
    "And finally the Foundations of Statistical Natural Language Processing (Manning). \n",
    "\n",
    "Examples of NLP: \n",
    "\n",
    "- Clustering News Articles\n",
    "- Suggesting similair books\n",
    "- Grouping Legal Docs\n",
    "- Analyzing Consumer Feedback\n",
    "- Spam Email Detection \n",
    "\n",
    "Basic process for NLP task concerns: \n",
    "\n",
    "- Compile all documents (Corpus)\n",
    "- Featurize the words to numerics\n",
    "- Compare features of documents\n",
    "\n",
    "We can distingush more complex or complicated ML algorithms like Words to Vector or WordToVec that is analyzing every sentence separetely. \n",
    "We are focusing on classic NLP here.\n",
    "\n",
    "The mechanism behind the NLP is based on TF-IDF method. \n",
    "That stands from Term Frequency - Inverse Document Frequency.\n",
    "\n",
    "Basicly it is creating \"Bag of Words\". Meaning it creates a word count vector array from the number of words showing in the particular doc. Again a bag of counted words, the most simple :) \n",
    "\n",
    "We can improve on Bag of Words by adjusting word counts based on their frequency in corpus(the group of all thedocuments). \n",
    "\n",
    "We can however improve on the TF-IDF method a lot!\n",
    "\n",
    "We end up doing is taking: \n",
    "\n",
    "- Term Frequency - Importance of the termin within the documnent. \n",
    " - TF(x,y) = Number of occurences of term x in document y\n",
    " \n",
    "- Inverse Document Frequency - Taking the length of doc into account.\n",
    "    - n total is a total number of docs\n",
    "    - the dfx stands for number of docs with the searched term. \n",
    "    - to achive the IDF(t) - we need to take the sum of n divided by dfx logarytmic value. \n",
    "    \n",
    "To sum it up the w = tf with x and y. Times logarythmic value of N divided by df. So the TF-IDF. \n",
    "\n",
    "In simple words we are using some tool to caount the values in the document. What we are taking into account is: \n",
    "how often particular term is shows up in the document. And then using the inverse document frequency the actuall differences \n",
    "between document lenghts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8576d395",
   "metadata": {},
   "source": [
    "# Spam detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79756887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/ubuntu/Spark/spark-3.3.0-bin-hadoop3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fe8bdc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/23 11:01:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('NLP').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a41b656c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c10657e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "376dd400",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b281b54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_df = spark.createDataFrame([\n",
    "    (0, 'Hi I heard about Spark'),\n",
    "    (1, 'I wish Java could use case classes'),\n",
    "    (2, 'Logistic,Regression,models,are,neat'),\n",
    "    \n",
    "], ['id', 'sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3bc4530e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|            sentence|\n",
      "+---+--------------------+\n",
      "|  0|Hi I heard about ...|\n",
      "|  1|I wish Java could...|\n",
      "|  2|Logistic,Regressi...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sen_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f3166bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol='sentence', outputCol='words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1cfde925",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_tokenzier = RegexTokenizer(inputCol='sentence', outputCol='words', pattern='\\\\W')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7ccc426",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tokens = udf(lambda words:len(words), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "650ab7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tokenizer.transform(sen_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8177a078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|            sentence|               words|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|Hi I heard about ...|[hi, i, heard, ab...|\n",
      "|  1|I wish Java could...|[i, wish, java, c...|\n",
      "|  2|Logistic,Regressi...|[logistic,regress...|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f1b90a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+------+\n",
      "| id|            sentence|               words|tokens|\n",
      "+---+--------------------+--------------------+------+\n",
      "|  0|Hi I heard about ...|[hi, i, heard, ab...|     5|\n",
      "|  1|I wish Java could...|[i, wish, java, c...|     7|\n",
      "|  2|Logistic,Regressi...|[logistic,regress...|     1|\n",
      "+---+--------------------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized.withColumn('tokens', count_tokens(col(\"words\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a40d471d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rg_tokenzie = regex_tokenzier.transform(sen_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba7eb3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+------+\n",
      "| id|            sentence|               words|tokens|\n",
      "+---+--------------------+--------------------+------+\n",
      "|  0|Hi I heard about ...|[hi, i, heard, ab...|     5|\n",
      "|  1|I wish Java could...|[i, wish, java, c...|     7|\n",
      "|  2|Logistic,Regressi...|[logistic, regres...|     5|\n",
      "+---+--------------------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rg_tokenzie.withColumn('tokens', count_tokens(col(\"words\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f69a6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4be5f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceDataFrame = spark.createDataFrame([\n",
    "    (0, [\"I\", \"saw\", \"green\", \"horse\"]),\n",
    "    (1, [\"Mary\", \"had\", \"a\", \"little\", \"lamb\"])\n",
    "    \n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
